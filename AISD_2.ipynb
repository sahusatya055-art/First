{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn4puhy6Is8HFVdbfB6wqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7c7d10b583e471286ad208619cf5bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51dba906718049229ba9e5fbe3753ba2",
              "IPY_MODEL_a22099add45a44659e38dce398373a98",
              "IPY_MODEL_ce8ea9f28acf4b7a98fa72ee0b7d35b9"
            ],
            "layout": "IPY_MODEL_c8ba07d016fe42acb0f87e2f69d0fb8f"
          }
        },
        "51dba906718049229ba9e5fbe3753ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ced021a0fce44521bba0b3fe72ab0a51",
            "placeholder": "​",
            "style": "IPY_MODEL_efe73316414f4686ac8d2e5e01339d85",
            "value": "model.safetensors: 100%"
          }
        },
        "a22099add45a44659e38dce398373a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac20266312454b96baca91a19113c1c1",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96f64b790dcb4d948f809570c569a311",
            "value": 352824413
          }
        },
        "ce8ea9f28acf4b7a98fa72ee0b7d35b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a87a3576c2ff42518dea02d8322e4bd5",
            "placeholder": "​",
            "style": "IPY_MODEL_6190026335af4e15901e47e9f1caf711",
            "value": " 353M/353M [00:04&lt;00:00, 85.0MB/s]"
          }
        },
        "c8ba07d016fe42acb0f87e2f69d0fb8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced021a0fce44521bba0b3fe72ab0a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe73316414f4686ac8d2e5e01339d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac20266312454b96baca91a19113c1c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f64b790dcb4d948f809570c569a311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a87a3576c2ff42518dea02d8322e4bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6190026335af4e15901e47e9f1caf711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8eeb56db4a9478fa0f5456ca5377933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fb69c677cc34707942e1f7a22ca1c01",
              "IPY_MODEL_47981f5f230f42618ee691b069979aa3",
              "IPY_MODEL_987e6eded9384c8081c5f6efc85308f1"
            ],
            "layout": "IPY_MODEL_a8bea62ad7c9463aa5a486f5f6890cd7"
          }
        },
        "4fb69c677cc34707942e1f7a22ca1c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17471d60605f4a70a817a58741c345e9",
            "placeholder": "​",
            "style": "IPY_MODEL_a714431e9a554ef683e8dce71c5a4ebf",
            "value": "generation_config.json: 100%"
          }
        },
        "47981f5f230f42618ee691b069979aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4fc2a6a75aa4d59a2e836d4e6ae3125",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2151c8854ee84a12b17761e05abf4493",
            "value": 124
          }
        },
        "987e6eded9384c8081c5f6efc85308f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32bb28e4308b4c43bca5bd01c524a50e",
            "placeholder": "​",
            "style": "IPY_MODEL_840d3d036f8041e0bb62dcd41a2767d8",
            "value": " 124/124 [00:00&lt;00:00, 7.25kB/s]"
          }
        },
        "a8bea62ad7c9463aa5a486f5f6890cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17471d60605f4a70a817a58741c345e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a714431e9a554ef683e8dce71c5a4ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fc2a6a75aa4d59a2e836d4e6ae3125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2151c8854ee84a12b17761e05abf4493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32bb28e4308b4c43bca5bd01c524a50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "840d3d036f8041e0bb62dcd41a2767d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahusatya055-art/First/blob/main/AISD_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Mini Assignments for Students\n",
        "New Polysemous Word\n",
        "Pick another word with multiple meanings (e.g., \"bat\" or \"apple\").\n",
        "\n",
        "Create two sentences where the word has different meanings.\n",
        "Use BERT to get token embeddings and compute cosine similarity between the two occurrences.\n",
        "Sentence Similarity with BERT\n",
        "\n",
        "Take 4 short sentences: two positive, two negative.\n",
        "Use [CLS] embeddings and build a 4×4 cosine similarity matrix.\n",
        "Are positive sentences closer to each other than to negative sentences?\n",
        "GPT Prompt Experiments\n",
        "\n",
        "Try 3 different prompts with DistilGPT2.\n",
        "For each, generate 2 samples (change top_k / top_p if you like).\n",
        "Observe how sensitive GPT is to the prompt wording.\n",
        "T5 Text Transformation\n",
        "\n",
        "Use t5-small with prompts like \"translate English to German:\" or \"summarize:\".\n",
        "Compare T5 outputs with GPT outputs on the same text.\n",
        "Concept Check (Short Written Answer)\n",
        "In your own words (5–6 lines each):\n",
        "\n",
        "Explain the difference between encoder-only, decoder-only, and encoder–decoder models.\n",
        "Why is BERT usually used for understanding tasks and not for long, coherent text generation?\n",
        "\n"
      ],
      "metadata": {
        "id": "35hMO6MV2g3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) New Polysemous Word\n",
        "Pick another word with multiple meanings (e.g., \"bat\" or \"apple\").\n",
        "\n",
        "Create two sentences where the word has different meanings.\n",
        "Use BERT to get token embeddings and compute cosine similarity between the two occurrences."
      ],
      "metadata": {
        "id": "XrnRiaL52kb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load BERT (DistilBERT)\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Two sentences with different meanings of \"bat\"\n",
        "s1 = \"The bat flew out of the cave at night.\"\n",
        "s2 = \"He hit the ball with a wooden bat.\"\n",
        "\n",
        "# Tokenize\n",
        "enc1 = tokenizer(s1, return_tensors=\"pt\")\n",
        "enc2 = tokenizer(s2, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    out1 = model(**enc1)\n",
        "    out2 = model(**enc2)\n",
        "\n",
        "# Token embeddings\n",
        "emb1 = out1.last_hidden_state[0]\n",
        "emb2 = out2.last_hidden_state[0]\n",
        "\n",
        "# Tokens\n",
        "tokens1 = tokenizer.convert_ids_to_tokens(enc1[\"input_ids\"][0])\n",
        "tokens2 = tokenizer.convert_ids_to_tokens(enc2[\"input_ids\"][0])\n",
        "\n",
        "# Get index of \"bat\"\n",
        "i1 = tokens1.index(\"bat\")\n",
        "i2 = tokens2.index(\"bat\")\n",
        "\n",
        "# Get embedding vectors\n",
        "v1 = emb1[i1]\n",
        "v2 = emb2[i2]\n",
        "\n",
        "# Cosine similarity\n",
        "sim = F.cosine_similarity(v1, v2, dim=0).item()\n",
        "\n",
        "print(\"Sentence 1 tokens:\", tokens1)\n",
        "print(\"Sentence 2 tokens:\", tokens2)\n",
        "print(\"\\nIndex in S1:\", i1, \"   Index in S2:\", i2)\n",
        "print(f\"\\nCosine similarity of 'bat' meanings: {sim:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9kqh6cJ2qMM",
        "outputId": "320ac721-f404-44d5-acd7-9e647fb80fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1 tokens: ['[CLS]', 'the', 'bat', 'flew', 'out', 'of', 'the', 'cave', 'at', 'night', '.', '[SEP]']\n",
            "Sentence 2 tokens: ['[CLS]', 'he', 'hit', 'the', 'ball', 'with', 'a', 'wooden', 'bat', '.', '[SEP]']\n",
            "\n",
            "Index in S1: 2    Index in S2: 8\n",
            "\n",
            "Cosine similarity of 'bat' meanings: 0.6763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Sentence Similarity with BERT\n",
        "\n",
        "Take 4 short sentences: two positive, two negative.\n",
        "Use [CLS] embeddings and build a 4×4 cosine similarity matrix.\n",
        "Are positive sentences closer to each other than to negative sentences?"
      ],
      "metadata": {
        "id": "RilBEmMx3c8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWCMXeTtbpVo",
        "outputId": "329cc041-ca65-42c3-9c06-d8d0492f2c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXwCrMmSbxT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "bert_name = \"distilbert-base-uncased\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
        "bert_model = AutoModel.from_pretrained(bert_name).to(device)\n",
        "bert_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onpQz9bzbtnP",
        "outputId": "8bad558f-dfeb-48b1-f0bd-5c61e6e060ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): DistilBertSdpaAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = bert_tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n"
      ],
      "metadata": {
        "id": "WoC2RecybysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 sentences: 2 positive, 2 negative\n",
        "sentences = [\n",
        "    \"I really enjoyed this movie.\",        # positive\n",
        "    \"The food was absolutely wonderful.\",  # positive\n",
        "    \"This movie was terrible and boring.\", # negative\n",
        "    \"I hated the food, it was awful.\"      # negative\n",
        "]\n",
        "\n",
        "# Tokenize all sentences\n",
        "enc = bert_tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "# Run through BERT\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(**enc)\n",
        "\n",
        "# Extract [CLS] embeddings (position 0)\n",
        "cls_embeddings = outputs.last_hidden_state[:, 0, :]   # shape: [4, hidden_dim]\n",
        "\n",
        "# Build 4×4 cosine similarity matrix\n",
        "sim_matrix = []\n",
        "for i in range(4):\n",
        "    row = []\n",
        "    for j in range(4):\n",
        "        sim = F.cosine_similarity(cls_embeddings[i], cls_embeddings[j], dim=0).item()\n",
        "        row.append(round(sim, 4))\n",
        "    sim_matrix.append(row)\n",
        "\n",
        "# Print the matrix\n",
        "print(\"Cosine Similarity Matrix (4×4):\\n\")\n",
        "for row in sim_matrix:\n",
        "    print(row)\n",
        "\n",
        "print(\"\\nSentences:\")\n",
        "for i, s in enumerate(sentences):\n",
        "    print(f\"{i+1}. {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfix8sWi3jIM",
        "outputId": "80dd38cb-495e-4a13-f75d-08acca0e3963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix (4×4):\n",
            "\n",
            "[1.0, 0.9727, 0.9719, 0.9615]\n",
            "[0.9727, 1.0, 0.9598, 0.964]\n",
            "[0.9719, 0.9598, 1.0, 0.9613]\n",
            "[0.9615, 0.964, 0.9613, 1.0]\n",
            "\n",
            "Sentences:\n",
            "1. I really enjoyed this movie.\n",
            "2. The food was absolutely wonderful.\n",
            "3. This movie was terrible and boring.\n",
            "4. I hated the food, it was awful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT Prompt Experiments\n",
        "\n",
        "Try 3 different prompts with DistilGPT2.\n",
        "For each, generate 2 samples (change top_k / top_p if you like).\n",
        "Observe how sensitive GPT is to the prompt wording."
      ],
      "metadata": {
        "id": "WkwNrPk4770M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "gpt_name = \"distilgpt2\"\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_name)\n",
        "\n",
        "# Fix GPT-2 pad token\n",
        "if gpt_tokenizer.pad_token is None:\n",
        "    gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(gpt_name).to(device)\n",
        "gpt_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550,
          "referenced_widgets": [
            "c7c7d10b583e471286ad208619cf5bed",
            "51dba906718049229ba9e5fbe3753ba2",
            "a22099add45a44659e38dce398373a98",
            "ce8ea9f28acf4b7a98fa72ee0b7d35b9",
            "c8ba07d016fe42acb0f87e2f69d0fb8f",
            "ced021a0fce44521bba0b3fe72ab0a51",
            "efe73316414f4686ac8d2e5e01339d85",
            "ac20266312454b96baca91a19113c1c1",
            "96f64b790dcb4d948f809570c569a311",
            "a87a3576c2ff42518dea02d8322e4bd5",
            "6190026335af4e15901e47e9f1caf711",
            "c8eeb56db4a9478fa0f5456ca5377933",
            "4fb69c677cc34707942e1f7a22ca1c01",
            "47981f5f230f42618ee691b069979aa3",
            "987e6eded9384c8081c5f6efc85308f1",
            "a8bea62ad7c9463aa5a486f5f6890cd7",
            "17471d60605f4a70a817a58741c345e9",
            "a714431e9a554ef683e8dce71c5a4ebf",
            "d4fc2a6a75aa4d59a2e836d4e6ae3125",
            "2151c8854ee84a12b17761e05abf4493",
            "32bb28e4308b4c43bca5bd01c524a50e",
            "840d3d036f8041e0bb62dcd41a2767d8"
          ]
        },
        "id": "jYfZVdOA8Qis",
        "outputId": "1e97ab52-38b2-44f8-80e3-1167526193b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7c7d10b583e471286ad208619cf5bed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8eeb56db4a9478fa0f5456ca5377933"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Artificial intelligence will\",\n",
        "    \"In 10 years, students will\",\n",
        "    \"The worst thing about social media is\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"PROMPT: {prompt}\\n\")\n",
        "\n",
        "    for i in range(2):\n",
        "        inputs = gpt_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = gpt_model.generate(\n",
        "                **inputs,\n",
        "                max_length=30,\n",
        "                do_sample=True,\n",
        "                top_k=50 if i == 0 else 10,\n",
        "                top_p=0.9 if i == 0 else 0.6,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=gpt_tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        generated = gpt_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(generated)\n",
        "        print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Observe how prompt wording changes GPT's responses.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J2t8KQb8Q2s",
        "outputId": "c45e9da1-3dfc-4755-a489-eaa410ed21b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PROMPT: Artificial intelligence will\n",
            "\n",
            "Sample 1:\n",
            "Artificial intelligence will make people more human, smarter and more intelligent.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sample 2:\n",
            "Artificial intelligence will be able to create a new kind of artificial intelligence.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROMPT: In 10 years, students will\n",
            "\n",
            "Sample 1:\n",
            "In 10 years, students will never be able to get into a job where they no longer have access to information, and to be taught that information is\n",
            "\n",
            "Sample 2:\n",
            "In 10 years, students will have to take part in the program, which will include a number of classes and classes, including the coursework, the\n",
            "\n",
            "================================================================================\n",
            "PROMPT: The worst thing about social media is\n",
            "\n",
            "Sample 1:\n",
            "The worst thing about social media is that most people will think it is a bad thing. I don't know if anyone will ever do that, but\n",
            "\n",
            "Sample 2:\n",
            "The worst thing about social media is that it is not a social media. It is a social media platform. It is not a social media platform.\n",
            "\n",
            "================================================================================\n",
            "Observe how prompt wording changes GPT's responses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)T5 Text Transformation\n",
        "\n",
        "Use t5-small with prompts like \"translate English to German:\" or \"summarize:\".\n",
        "Compare T5 outputs with GPT outputs on the same text."
      ],
      "metadata": {
        "id": "4-slfNoGU1Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFmdJ_ZJU1Sv",
        "outputId": "2796b821-f056-4d6b-f005-bb53078df9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "t5_name = \"t5-small\"\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(t5_name)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)\n",
        "t5_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mCyZKwWqU1mf",
        "outputId": "4b20613e-54b7-4aa2-bdbb-c61522f0ea0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Machine learning allows computers to learn from data and improve their performance over time.\"\n",
        "print(\"ORIGINAL TEXT:\\n\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzcAy898WgZ3",
        "outputId": "ba3ed72c-b389-4536-ea12-91eb425c69af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL TEXT:\n",
            " Machine learning allows computers to learn from data and improve their performance over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t5_input_trans = \"translate English to German: \" + text\n",
        "\n",
        "enc_trans = t5_tokenizer(t5_input_trans, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    trans_ids = t5_model.generate(\n",
        "        **enc_trans,\n",
        "        max_length=60,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "t5_german = t5_tokenizer.decode(trans_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nT5 TRANSLATION (EN → DE):\\n\", t5_german)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdbH3wL1Wgs_",
        "outputId": "3f5fad8e-e9e4-4697-a06f-d66477935839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "T5 TRANSLATION (EN → DE):\n",
            " Automatisches Lernen ermöglicht es Computern, aus Daten zu lernen und ihre Leistung im Laufe der Zeit zu verbessern.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: GPT – Try to translate the same text"
      ],
      "metadata": {
        "id": "BmO3gpcKZZoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_prompt_trans = f\"Translate this English sentence to German:\\n{text}\\nGerman:\"\n",
        "\n",
        "inputs = gpt_tokenizer(gpt_prompt_trans, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    gpt_ids = gpt_model.generate(\n",
        "        **inputs,\n",
        "        max_length=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        pad_token_id=gpt_tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "gpt_output = gpt_tokenizer.decode(gpt_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGPT OUTPUT (Translation-style prompt):\\n\", gpt_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf6EZwpLZag3",
        "outputId": "6a2e207f-b29a-47ce-b45c-c4ba10271502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT OUTPUT (Translation-style prompt):\n",
            " Translate this English sentence to German:\n",
            "Machine learning allows computers to learn from data and improve their performance over time.\n",
            "German: machine learning is the key to a successful machine learning project.\n",
            "Learn more about Machine Learning at www.machinelearning.org.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Concept Check (Short Written Answer)\n",
        "In your own words (5–6 lines each):\n",
        "\n",
        "Explain the difference between encoder-only, decoder-only, and encoder–decoder models.\n",
        "Why is BERT usually used for understanding tasks and not for long, coherent text generation?\n",
        "\n"
      ],
      "metadata": {
        "id": "GFnxSplfdHse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Difference between encoder-only, decoder-only, and encoder–decoder models (5–6 lines)\n",
        "\n",
        "Encoder-only models (like BERT) read the entire input at once using bidirectional attention, which helps them understand context deeply. They are mainly used for tasks like classification, similarity, and question answering.\n",
        "Decoder-only models (like GPT) generate text one token at a time and only look at previous tokens (left-to-right). This makes them very good at text generation and prediction.\n",
        "Encoder–decoder models (like T5 or BART) use an encoder to understand the input and a decoder to produce an output. They are ideal for tasks like translation, summarization, and text transformation.\n",
        "\n"
      ],
      "metadata": {
        "id": "h3_MtPuaZax-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why BERT is used for understanding, not long text generation (5–6 lines)\n",
        "\n",
        "BERT is an encoder-only model, so it processes all words together and looks both left and right (bidirectional). Because of this, it cannot generate text step-by-step like GPT does.\n",
        "BERT is designed to understand meaning, classify text, or produce embeddings—not to continue sentences.\n",
        "Long, coherent text generation requires an auto-regressive decoder that predicts the next token, which BERT does not have.\n",
        "This is why GPT-style models are used for generation, while BERT excels in comprehension tasks."
      ],
      "metadata": {
        "id": "tdMH-MKMZbB_"
      }
    }
  ]
}